{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "# wordnet = nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "import pickle"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "DATA_DIR = '../data/'\n",
    "TRAIN_FILE = 'test_tweets.csv'\n",
    "TEST_FILE = 'test_tweets.csv'\n",
    "APOSTROPHE_DICT = {\n",
    "    \"ain't\": \"am not / are not\",\n",
    "    \"aren't\": \"are not / am not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"can't've\": \"cannot have\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"couldn't've\": \"could not have\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hadn't've\": \"had not have\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he had / he would\",\n",
    "    \"he'd've\": \"he would have\",\n",
    "    \"he'll\": \"he shall / he will\",\n",
    "    \"he'll've\": \"he shall have / he will have\",\n",
    "    \"he's\": \"he has / he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'd'y\": \"how do you\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how has / how is\",\n",
    "    \"i'd\": \"I had / I would\",\n",
    "    \"i'd've\": \"I would have\",\n",
    "    \"i'll\": \"I shall / I will\",\n",
    "    \"i'll've\": \"I shall have / I will have\",\n",
    "    \"i'm\": \"I am\",\n",
    "    \"i've\": \"I have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it had / it would\",\n",
    "    \"it'd've\": \"it would have\",\n",
    "    \"it'll\": \"it shall / it will\",\n",
    "    \"it'll've\": \"it shall have / it will have\",\n",
    "    \"it's\": \"it has / it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mightn't've\": \"might not have\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"needn't've\": \"need not have\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"oughtn't've\": \"ought not have\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"sha'n't\": \"shall not\",\n",
    "    \"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she had / she would\",\n",
    "    \"she'd've\": \"she would have\",\n",
    "    \"she'll\": \"she shall / she will\",\n",
    "    \"she'll've\": \"she shall have / she will have\",\n",
    "    \"she's\": \"she has / she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\",\n",
    "    \"so've\": \"so have\",\n",
    "    \"so's\": \"so as / so is\",\n",
    "    \"that'd\": \"that would / that had\",\n",
    "    \"that'd've\": \"that would have\",\n",
    "    \"that's\": \"that has / that is\",\n",
    "    \"there'd\": \"there had / there would\",\n",
    "    \"there'd've\": \"there would have\",\n",
    "    \"there's\": \"there has / there is\",\n",
    "    \"they'd\": \"they had / they would\",\n",
    "    \"they'd've\": \"they would have\",\n",
    "    \"they'll\": \"they shall / they will\",\n",
    "    \"they'll've\": \"they shall have / they will have\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"to've\": \"to have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we had / we would\",\n",
    "    \"we'd've\": \"we would have\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we'll've\": \"we will have\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what shall / what will\",\n",
    "    \"what'll've\": \"what shall have / what will have\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what has / what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"when's\": \"when has / when is\",\n",
    "    \"when've\": \"when have\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where has / where is\",\n",
    "    \"where've\": \"where have\",\n",
    "    \"who'll\": \"who shall / who will\",\n",
    "    \"who'll've\": \"who shall have / who will have\",\n",
    "    \"who's\": \"who has / who is\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"why's\": \"why has / why is\",\n",
    "    \"why've\": \"why have\",\n",
    "    \"will've\": \"will have\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"wouldn't've\": \"would not have\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"y'all'd\": \"you all would\",\n",
    "    \"y'all'd've\": \"you all would have\",\n",
    "    \"y'all're\": \"you all are\",\n",
    "    \"y'all've\": \"you all have\",\n",
    "    \"you'd\": \"you had / you would\",\n",
    "    \"you'd've\": \"you would have\",\n",
    "    \"you'll\": \"you shall / you will\",\n",
    "    \"you'll've\": \"you shall have / you will have\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\"\n",
    "}\n",
    "SHORT_WORD_DICT = {\n",
    "    \"121\": \"one to one\",\n",
    "    \"a/s/l\": \"age, sex, location\",\n",
    "    \"adn\": \"any day now\",\n",
    "    \"afaik\": \"as far as I know\",\n",
    "    \"afk\": \"away from keyboard\",\n",
    "    \"aight\": \"alright\",\n",
    "    \"alol\": \"actually laughing out loud\",\n",
    "    \"b4\": \"before\",\n",
    "    \"b4n\": \"bye for now\",\n",
    "    \"bak\": \"back at the keyboard\",\n",
    "    \"bf\": \"boyfriend\",\n",
    "    \"bff\": \"best friends forever\",\n",
    "    \"bfn\": \"bye for now\",\n",
    "    \"bg\": \"big grin\",\n",
    "    \"bta\": \"but then again\",\n",
    "    \"btw\": \"by the way\",\n",
    "    \"cid\": \"crying in disgrace\",\n",
    "    \"cnp\": \"continued in my next post\",\n",
    "    \"cp\": \"chat post\",\n",
    "    \"cu\": \"see you\",\n",
    "    \"cul\": \"see you later\",\n",
    "    \"cul8r\": \"see you later\",\n",
    "    \"cya\": \"bye\",\n",
    "    \"cyo\": \"see you online\",\n",
    "    \"dbau\": \"doing business as usual\",\n",
    "    \"fud\": \"fear, uncertainty, and doubt\",\n",
    "    \"fwiw\": \"for what it's worth\",\n",
    "    \"fyi\": \"for your information\",\n",
    "    \"g\": \"grin\",\n",
    "    \"g2g\": \"got to go\",\n",
    "    \"ga\": \"go ahead\",\n",
    "    \"gal\": \"get a life\",\n",
    "    \"gf\": \"girlfriend\",\n",
    "    \"gfn\": \"gone for now\",\n",
    "    \"gmbo\": \"giggling my butt off\",\n",
    "    \"gmta\": \"great minds think alike\",\n",
    "    \"h8\": \"hate\",\n",
    "    \"hagn\": \"have a good night\",\n",
    "    \"hdop\": \"help delete online predators\",\n",
    "    \"hhis\": \"hanging head in shame\",\n",
    "    \"iac\": \"in any case\",\n",
    "    \"ianal\": \"I am not a lawyer\",\n",
    "    \"ic\": \"I see\",\n",
    "    \"idk\": \"I don't know\",\n",
    "    \"imao\": \"in my arrogant opinion\",\n",
    "    \"imnsho\": \"in my not so humble opinion\",\n",
    "    \"imo\": \"in my opinion\",\n",
    "    \"iow\": \"in other words\",\n",
    "    \"ipn\": \"I’m posting naked\",\n",
    "    \"irl\": \"in real life\",\n",
    "    \"jk\": \"just kidding\",\n",
    "    \"l8r\": \"later\",\n",
    "    \"ld\": \"later, dude\",\n",
    "    \"ldr\": \"long distance relationship\",\n",
    "    \"llta\": \"lots and lots of thunderous applause\",\n",
    "    \"lmao\": \"laugh my ass off\",\n",
    "    \"lmirl\": \"let's meet in real life\",\n",
    "    \"lol\": \"laugh out loud\",\n",
    "    \"ltr\": \"longterm relationship\",\n",
    "    \"lulab\": \"love you like a brother\",\n",
    "    \"lulas\": \"love you like a sister\",\n",
    "    \"luv\": \"love\",\n",
    "    \"m/f\": \"male or female\",\n",
    "    \"m8\": \"mate\",\n",
    "    \"milf\": \"mother I would like to fuck\",\n",
    "    \"oll\": \"online love\",\n",
    "    \"omg\": \"oh my god\",\n",
    "    \"otoh\": \"on the other hand\",\n",
    "    \"pir\": \"parent in room\",\n",
    "    \"ppl\": \"people\",\n",
    "    \"r\": \"are\",\n",
    "    \"rofl\": \"roll on the floor laughing\",\n",
    "    \"rpg\": \"role playing games\",\n",
    "    \"ru\": \"are you\",\n",
    "    \"shid\": \"slaps head in disgust\",\n",
    "    \"somy\": \"sick of me yet\",\n",
    "    \"sot\": \"short of time\",\n",
    "    \"thanx\": \"thanks\",\n",
    "    \"thx\": \"thanks\",\n",
    "    \"ttyl\": \"talk to you later\",\n",
    "    \"u\": \"you\",\n",
    "    \"ur\": \"you are\",\n",
    "    \"uw\": \"you’re welcome\",\n",
    "    \"wb\": \"welcome back\",\n",
    "    \"wfm\": \"works for me\",\n",
    "    \"wibni\": \"wouldn't it be nice if\",\n",
    "    \"wtf\": \"what the fuck\",\n",
    "    \"wtg\": \"way to go\",\n",
    "    \"wtgp\": \"want to go private\",\n",
    "    \"ym\": \"young man\",\n",
    "    \"gr8\": \"great\"\n",
    "}\n",
    "EMOTICON_DICT = {\n",
    "    \":)\": \"happy\",\n",
    "    \":‑)\": \"happy\",\n",
    "    \":-]\": \"happy\",\n",
    "    \":-3\": \"happy\",\n",
    "    \":->\": \"happy\",\n",
    "    \"8-)\": \"happy\",\n",
    "    \":-}\": \"happy\",\n",
    "    \":o)\": \"happy\",\n",
    "    \":c)\": \"happy\",\n",
    "    \":^)\": \"happy\",\n",
    "    \"=]\": \"happy\",\n",
    "    \"=)\": \"happy\",\n",
    "    \"<3\": \"happy\",\n",
    "    \":-(\": \"sad\",\n",
    "    \":(\": \"sad\",\n",
    "    \":c\": \"sad\",\n",
    "    \":<\": \"sad\",\n",
    "    \":[\": \"sad\",\n",
    "    \">:[\": \"sad\",\n",
    "    \":{\": \"sad\",\n",
    "    \">:(\": \"sad\",\n",
    "    \":-c\": \"sad\",\n",
    "    \":-< \": \"sad\",\n",
    "    \":-[\": \"sad\",\n",
    "    \":-||\": \"sad\"\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(f'{DATA_DIR}{TRAIN_FILE}', index_col='id')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(f'{DATA_DIR}{TEST_FILE}', index_col='id')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "df = pd.concat([train_df, test_df])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "v_run_sub = np.vectorize(re.sub)\n",
    "\n",
    "def change_string(map_dict: dict, text: str) -> str:\n",
    "    for w in text.split():\n",
    "        if repl := map_dict.get(w):\n",
    "            text = v_run_sub(re.escape(w),repl, text)\n",
    "\n",
    "    return text\n",
    "\n",
    "v_change_string = np.vectorize(change_string)\n",
    "\n",
    "def delete_one_char(text: str) -> str:\n",
    "\n",
    "    return ' '.join([w for w in text.split() if len(w)>1])\n",
    "\n",
    "v_delete_one_char = np.vectorize(delete_one_char)\n",
    "\n",
    "stopworlds = nltk.corpus.stopwords.words('english')\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "df['clean_tweet'] = v_run_sub(r\"@[\\w]*\", ' ', df['tweet'])\n",
    "df['clean_tweet'] = df['clean_tweet'].str.lower()\n",
    "df['clean_tweet'] = v_change_string(APOSTROPHE_DICT, df['clean_tweet'])\n",
    "df['clean_tweet'] = v_change_string(SHORT_WORD_DICT, df['clean_tweet'])\n",
    "df['clean_tweet'] = v_change_string(EMOTICON_DICT, df['clean_tweet'])\n",
    "df['clean_tweet'] = v_run_sub(r'[^\\w\\s]', ' ', df['clean_tweet'])\n",
    "df['clean_tweet'] = v_run_sub(r'[^a-zA-Z0-9]', ' ', df['clean_tweet'])\n",
    "df['clean_tweet'] = v_run_sub(r'[^a-zA-Z]', ' ', df['clean_tweet'])\n",
    "df['clean_tweet'] = v_delete_one_char(df['clean_tweet'])\n",
    "df['tweet_token'] = df['clean_tweet'].apply(lambda x: nltk.tokenize.word_tokenize(x))\n",
    "df['tweet_token_filtered'] = df['tweet_token'].apply(lambda x: [w for w in x if w not in stopworlds])\n",
    "df['tweet_stemmed'] = df['tweet_token_filtered'].apply(lambda x: [stemmer.stem(w) for w in x])\n",
    "df['tweet_lemmatized'] = df['tweet_stemmed'].apply(lambda x: [lemmatizer.lemmatize(w, wordnet.VERB) for w in x])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "with open('data.pickle', 'wb') as f:\n",
    "    pickle.dump(df, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}